"""
Specifies functions to calculate the messages generated by different basic NUV 
priors. These messages can either be specified by their mean and covariance 
matrix representatons, or their dual mean and precision matrix representations. 
Later is sometimes refered to as the 'dual' representation. The quantity to 
which these priors are applied is called X.
"""

import numpy as np


def laplace(
        mx_hat: np.ndarray, beta: float, inverse: bool=False) -> np.ndarray:
    """
    Calculates messages generated by laplace NUV prior for D-dimensional data. 
    For beta>0, the prior is sparsifying. For beta<0, the prior is repulsive. 
    Note that the generated mean (and dual mean) messages are always zero and 
    therefore omitted.
    
    Args:
        mx_hat (np.ndarray): Mean estimates of X from previous iteration.
                .shape=(N,D)
        beta (float): Tuning parameter for NUV update. Higher values 
            correspond to a more aggressive prior.
        inverse (bool): If False, the representation of the outgoing 
            messages is in terms of their covariance matrices. If True, 
            their inverses (i.e., the precision matrices) are given. 
            Default is False.
            
    Returns:
        VWx_out (np.ndarray): Outgoing messages either by their covariance 
            or precision matrix representation.
                .shape=(N,D,D)
    """
    
    # Get dimensions
    _,D = mx_hat.shape
    
    # Calculate (scalar) update for laplace NUV
    varx = np.linalg.norm(mx_hat, axis=1) / beta
    
    # Construct outgoing precision matrix message
    if inverse:
        VWx_out = np.array([
            np.identity(D, dtype=float)*1/varu_i for varu_i in varx])
        
    # Construct outgoing covariance matrix message
    else:
        VWx_out = np.array([
            np.identity(D, dtype=float)*varu_i for varu_i in varx])
        
    return VWx_out

def logCost(
        mx_hat: np.ndarray, Vx_hat: np.ndarray, beta: float, 
        inverse: bool=False, updateType: str='EM') -> np.ndarray:
    """
    Calculates messages generated by log-cost NUV for D-dimensional data. For 
    beta>0, the prior is sparcifying. For beta<0, the prior is repulsive. Note 
    that the generated mean (and dual mean) messages are always zero and 
    therefore omitted. For beta=D, this prior is also called the 'plain-NUV'.
    
    Args:
        mx_hat (np.ndarray): Mean estimates of X from previous iteration.
                .shape=(N,D)
        Vx_hat (np.ndarray): Covariance estimates of X from previous iteration.
                .shape=(N,D,D)
        beta (float): Tuning parameter for NUV update. Higher values 
            correspond to a more aggressive prior.
        inverse (bool): If False, the representation of the outgoing 
            messages is in terms of their covariance matrices. If True, 
            their inverses (i.e., the precision matrices) are given. 
            Default is False.
        updateType (str): Specifies the NUV update type, must be in 
            ['AM', 'EM']. Default is 'EM'.
            
    Returns:
        VWx_out (np.ndarray): Outgoing messages either by their covariance 
            or precision matrix representation.
                .shape=(N,D,D)
    """
    
    valid_updateType = ['AM', 'EM']
    assert updateType in ['AM', 'EM'], \
        f'updateType must be in {valid_updateType}, ' + \
        f'updateType={updateType} is not known!'
    
    # Get dimensions
    _,D = mx_hat.shape
    
    # Calculate (scalar) update for log-cost NUV
    if updateType == 'EM':
        varx = \
            (np.trace(Vx_hat, axis1=1, axis2=2) + \
             np.linalg.norm(mx_hat, axis=1)**2) / beta
    else:
        varx = np.linalg.norm(mx_hat, axis=1)**2 / beta
    
    # Construct outgoing precision matrix message
    if inverse:
        VWx_out = np.array([
            np.identity(D, dtype=float)*1/varu_i for varu_i in varx])
        
    # Construct outgoing covariance matrix message
    else:
        VWx_out = np.array([
            np.identity(D, dtype=float)*varu_i for varu_i in varx])
        
    return VWx_out

def discreteBase(
        mx_hat: np.ndarray, Vx_hat: np.ndarray, inverse: bool=False, 
        updateType: str='EM'
        ) -> tuple[np.ndarray, np.ndarray]:
    """
    Calculates messages generated by a discrete-phase NUV for D-dimensional 
    data. Its target vectors are set to the base vectors, causing its cost 
    function to approach minus infinity at the points 
    [1,0,...], [0,1,...], ... . This prior does not have a "tuning factor" 
    similar to the laplace or logCost prior. Furthermore, it can only be used 
    in an attracting fashion (i.e., positive beta).
    
    Args:
        mx_hat (np.ndarray): Mean estimates of X from previous iteration.
                .shape=(N,D)
        Vx_hat (np.ndarray): Covariance estimates of X from previous iteration.
                .shape=(N,D,D)
        inverse (bool): If False, the representation of the outgoing 
            messages is in terms of their covariance matrices. If True, 
            their inverses (i.e., the precision matrices) are given. 
            Default is False.
        updateType (str): Specifies the NUV update type, must be in 
            ['AM', 'EM']. 
            
    Returns:
        mxix_out (np.ndarray): Outgoing messages either by their mean or dual 
            mean representation.
                .shape=(N,D)
        VWx_out (np.ndarray): Outgoing messages either by their covariance 
            or precision matrix representation.
                .shape=(N,D,D)
    """
    
    valid_updateType = ['AM', 'EM']
    assert updateType in ['AM', 'EM'], \
        f'updateType must be in {valid_updateType}, ' + \
        f'updateType={updateType} is not known!'
    
    # Get dimensions
    N,D = mx_hat.shape
    
    # Construct target vectors t_m
    t_m = np.identity(D, dtype=float)
    
    # Calculate / construct xix_out
    xix_out = np.empty((N,D), dtype=float)   # .shape=(N,D)
    if updateType == 'EM':
        Vx_hat_trace = np.trace(Vx_hat, axis1=1, axis2=2)
        for m in range(D):
            xix_out[:,m] = \
                D / \
                (Vx_hat_trace + 
                 np.linalg.norm(mx_hat - np.tile(t_m[m], (N,1)), axis=1)**2)
    else:
        for m in range(D):
            xix_out[:,m] = \
                D / np.linalg.norm(mx_hat - np.tile(t_m[m], (N,1)), axis=1)**2
            
        
    # Calculate precx, i.e., the scalar by which the precision matrices have 
    # to be scaled
    precx = np.sum(xix_out, axis=1)  # .shape=N
    
    # Calculate / construct outgoing dual mean and precision messages
    if inverse:
        mxix_out = xix_out
        VWx_out = np.array([
            np.identity(D, dtype=float)*precx_i for precx_i in precx])
        
    # Calculate / construct outgoing mean and covariance messages
    else:
        varx = 1/precx
        VWx_out = np.array([
            np.identity(D, dtype=float)*varx_i for varx_i in varx])
        mxix_out = np.reshape(varx, (N,1)) * xix_out
        
    return mxix_out, VWx_out

def boxCostPositivity(
        mx_hat: np.ndarray, beta: float, inverse: bool=False
        ) -> tuple[np.ndarray, np.ndarray]: 
    """
    Calculates messages generated by a box-constraint NUV for D-dimensional 
    data. The borders of the box-constraint are chosen to enforce positivity, 
    i.e., constrainted to R_+^D. It can only be used in an attracting fashion 
    (i.e., non-negative beta).
    
    Args:
        mx_hat (np.ndarray): Mean estimates of X from previous iteration.
                .shape=(N,D)
        beta (float): Tuning parameter for NUV update. Higher values 
            correspond to a more aggressive prior.
        inverse (bool): If False, the representation of the outgoing 
            messages is in terms of their mean and covariance matrices. If 
            True, their inverses (i.e., the dual mean and precision matrices) 
            are given. Default is False.
            
    Returns:
        mxix_out (np.ndarray): Outgoing messages either by their mean or dual 
            mean representation.
                .shape=(N,D)
        VWx_out (np.ndarray): Outgoing messages either by their covariance 
            or precision matrix representation.
                .shape=(N,D,D)
    """
    
    assert beta >= 0.0, f'beta must be non-negative for any box prior!'
    
    # Get dimensions
    N,D = mx_hat.shape
    
    # Calculate / construct outgoing dual mean and precision messages
    if inverse:
        mxix_out = np.full((N,D), beta, dtype=float)    # .shape=(N,M)
        mx_out = np.abs(mx_hat)
        VWx_out = np.array([np.diag(beta / mx_out_i) for mx_out_i in mx_out])   
            # .shape=(N,M,M)
    
    # Calculate / construct outgoing mean and covariance messages
    else:
        mxix_out = np.abs(mx_hat)   # .shape=(N,M)
        VWx_out = np.array([
            np.diag(mxix_out_i / beta) for mxix_out_i in mxix_out])
            # .shape=(N,M,M)
    
    return mxix_out, VWx_out