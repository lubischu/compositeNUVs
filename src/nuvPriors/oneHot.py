"""
Specifies function to calculate messages generated by One-Hot NUV constraint. 
"""

import numpy as np
from nuvPriors_basic import laplace, logCost, discreteBase

def oneHot(
        mx_hat: np.ndarray, Vx_hat: np.ndarray, beta: float, 
        sh_squared: float=1e-6, priorType: str='repulsive_logCost', 
        inverse: bool=False) -> tuple[np.ndarray, np.ndarray]:
    """
    Calculates messages generated by the One-Hot NUV prior for D-dimensional 
    data. The used repulsive prior can be selected by priorType. For numerical 
    stability, one is highly encouraged to choose sh_squared>0. Note that EM 
    updates are used where ever possible.
    
    Args:
        mx_hat (np.ndarray): Mean estimates of X from previous iteration.
                .shape=(N,M)
        Vx_hat (np.ndarray): Covariance estimates of X from previous iteration.
                .shape=(N,M,M)
        beta (float): Tuning parameter for repulsive NUV prior. Its value must 
            be positive! Note that the discreteBase NUV can not be tuned.
        sh_squared (float): Uncertainty / slack associated with condition 
            that sum over X must be one. Should be chosen small (i.e., 
            close to zero). Equal to zero should work too, but tends to
            be quite unstable.
        priorType (str): Specifies type of repulsive NUV prior used. Possible 
            cases are ['sparse', 'repulsive_laplace', 'repulsive_logCost', 
            'discrete']. They correspond to
                'sparse': Sparcifying log-cost prior per dimension.
                'repulsive_laplace': Sparcifying D-dimensional laplace prior.
                'repulsive_logCost': Sparcifying D-dimensional log-cost prior.
                'discrete': Discrete base-vectors, relying on idea of 
                    discrete-phase prior.
        inverse (bool): If False, the representation of the outgoing 
            messages is in terms of their mean and covariance matrices. If 
            True, their inverses (i.e., the dual mean and precision matrices) 
            are given. Default is False.
        
    Returns: 
        mxix_out (np.ndarray): Outgoing messages either by their mean or dual 
            mean representation.
                .shape=(N,M)
        VWx_out (np.ndarray): Outgoing messages either by their covariance 
            or precision matrix representation.
                .shape=(N,M,M)
    """
    
    valid_priorType = \
        ['sparse', 'repulsive_laplace', 'repulsive_logCost', 'discrete']
    assert priorType in valid_priorType, \
        f'priorType={priorType} is unknown! Valid prior types are ' + \
        f'{valid_priorType}'
    assert beta >= 0.0, \
        f'beta must be chosen positive, even though it is used for a ' + \
        f'repulsive prior!'
    
    # Get dimensions
    N,M = mx_hat.shape
    
    # Calculate outgoing dual mean and precision messages
    if inverse:
        
        # Calculate messages generated by repulsive prior for selected prior 
        # type

        if priorType == 'sparse':
            xixpp_f = np.full((N,M), 0.0, dtype=float)
            Wxpp_f_vec = \
                beta / (mx_hat**2 + np.diagonal(Vx_hat, axis1=1, axis2=2))
                # .shape=(N,M)
            Wxpp_f = np.array([
                np.diag(Whpp_f_vec_i) for Whpp_f_vec_i in Wxpp_f_vec])
                # .shape=(N,M,M)

        elif priorType == 'repulsive_laplace':
            xixpp_f = np.full((N,M), 0.0, dtype=float)
            Wxpp_f = laplace(mx_hat=mx_hat, beta=-beta, inverse=True)

        elif priorType == 'repulsive_logCost':
            xixpp_f = np.full((N,M), 0.0, dtype=float)
            Wxpp_f = logCost(
                mx_hat=mx_hat, Vx_hat=Vx_hat, beta=-beta, inverse=True)

        elif priorType == 'discrete':
            xixpp_f, Wxpp_f = discreteBase(
                mx_hat=mx_hat, Vx_hat=Vx_hat, inverse=True)
            
        # Calculate resulting messages
        mxix_out = np.full((N,M), 1/sh_squared, dtype=float) + xixpp_f
        VWx_out = np.full((N,M,M), 1/sh_squared, dtype=float) + Wxpp_f
    
    # Calculate outgoing mean and covariance messages
    else:
        
        # Calculate messages out of repulsive prior for selected repulsive 
        # prior (in priorType)

        if priorType == 'sparse':
            mxpp_f = np.full((N,M), 0.0, dtype=float)
            Vxpp_f_vec = \
                (mx_hat**2 + np.diagonal(Vx_hat, axis1=1, axis2=2)) / \
                beta   # .shape=(N,M)
            Vxpp_f = np.array([
                np.diag(Vhpp_f_vec_i) for Vhpp_f_vec_i in Vxpp_f_vec])
                # .shape=(N,M,M)

        elif priorType == 'repulsive_laplace':
            mxpp_f = np.full((N,M), 0.0, dtype=float)
            Vxpp_f = laplace(mx_hat=mx_hat, beta=-beta, inverse=False)

        elif priorType == 'repulsive_logCost':
            mxpp_f = np.full((N,M), 0.0, dtype=float)
            Vxpp_f = logCost(
                mx_hat=mx_hat, Vx_hat=Vx_hat, beta=-beta, inverse=False)

        elif priorType == 'discrete':
            mxpp_f, Vxpp_f = discreteBase(
                mx_hat=mx_hat, Vx_hat=Vx_hat, inverse=False)
        
        # Calculate G_i 
        G = 1 / (sh_squared + np.trace(Vxpp_f, axis1=1, axis2=2))   # .shape=N
        
        # Calculate resulting messages
        mxix_out = \
            mxpp_f + \
            np.reshape(
                Vxpp_f @ 
                np.full((N,M,1), np.reshape(G, (N,1,1))) * 
                np.reshape(
                    np.full(N, 1.0, dtype=float) - np.sum(mxpp_f, axis=1), 
                    (N,1,1)), 
                (N,M))   # .shape=(N,M)
        VWx_out = \
            Vxpp_f - Vxpp_f @ np.full((N,M,M), np.reshape(G, (N,1,1))) @ Vxpp_f
            # .shape=(N,M,M)

    return mxix_out, VWx_out